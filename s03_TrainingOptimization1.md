# Training Optimization 1
In general, an optimization problem is the problem of finding an optimal value $x$ of a function $f(x)$ by maximizing or minimizing this function $f$. In the context of neural network training, optimization is the process 
of minimization of the loss function and accordingly, updating the parameters of the model such that the output accuracy of the neural network is maximized.    
Training neural networks is the most difficult optimization involved in deep learning and it differs from pure optimization in different ways.
 The cost function is usually non-convex which causes several problems and requires a careful choice of initial points. 
To conquer these problems, several special optimization algorithms have been developed. The algorithms that are covered here are first order methods.

## 8.1 How Learning Differs from Pure Optimization

In machine learning, we usually want to optimize a performance measure P with respect to the test set. As P can only be optimized indirectly (in contrast to pure optimization, where we directly optimize a term of interest) and we do not know the underlying probability distribution of the data, the problem we need to solve is minimizing the empirical risk $E_{(x,y)\sim \hat{p}_data}[L(f(x;θ),y)] = \frac{1}{m} \sum_{i=1}^{m}L(f(x;θ),y),$ where $\hat{p}_data$ is the empirical distribution, L the loss function, f the predicted output for input x and y the actual output. Here, we will only look at the unregularized supervised case. <br />
Empirical risk minimization is rarely used in deep learning, because the loss functions do not have useful derivatives in many cases and it is likely that overfitting occurs.

Instead of the actual loss function we often minimize a surrogate loss function, which acts as a proxy and has more suitable properties for optimization. Minimizing the surrogate loss function halts when early stopping criterion is met. In particular, this means that training often halts when surrogate loss function still has large derivatives. Which is an other difference to pure optimization where we require the gradient to be zero at convergence. The early stopping criterion is based on true underlying loss function measured on the validation set.

An other difference to pure optimization is that in machine learning the objective function usually decomposes as a sum over training examples. We compute each update to the parameters based on an expected value of the cost function only on a small subset of the terms of the full cost function as computing the expectation on the whole dataset is very expensive.

In deep learning, we the optimization algorithms we use are usually so called minibatch or stochastic algorithms. That means we train our model on a batch of examples that is greater than one but also smaller than the whole training set. <br />
When we pick the minibatches, we have to consider the following points: The minibatches have to be selected randomly and subsequent minibatches should be independent of each other in order to get unbiased estimates that are independent of each other. Also, we have to shuffle examples if ordering is significant. In the special case of very large datasets the minibatches are constructed from shuffled examples rather than selected randomly. <br />
Factors influencing the size are: How accurate we want the estimate to be (larger batches yield more accurate estimates), the trade-off between regularization and  optimization, hardware and memory limitations and that multicore architectures are underutilized by very small batches, so it might make sense to define a minimum batch size.

## 8.2 Challenges in Neural Network Optimization

Several challenges arise when optimizing neural networks. Some of them are related to critical points on the surface of the underlying loss function, others are caused by the architecture of the deep neural network.
 These challenges complicate the optimization process during neural networks training, however, there are techniques that help to overcome their limitations.
 In this section, some challenges facing the optimization process are presented as well as their mitigation techniques. <br />

### Definitions (Recap) <br />
Optimizing the training process of neural networks consists of finding the global minimum of the loss function and altering the parameters of the model based on this finding. In general, loss functions of deep neural networks show nonconvex surfaces consisting of a single global minimum and a huge number of local minima. The optimization process on such complex surfaces starts from a point and descends all along the surface of the cost function until the global minimum or a satisfying local minimum is reached. <br />
Formally, let $L(f(x;\theta), y)$ be the loss at point $x$, where $f(x;\theta)$ is the prediction delivered by the neural network, $\theta$ is the set of parameters (e.g. weights) of the deep model and $y$ is the true label of the input $x$. The goal is to reach a point $x^{\ast}$ such that $L(f(x^{\ast};\theta), y)$ is minimal. Starting from an arbitrary point $x$ on the cost function $L$, the first order partial derivative with respect to $x$, $\frac{\partial L}{\partial x}$, is calculated to determine the slope of the loss function at point $x$. According to this slope, the gradient descent optimization method is applied iteratively on the cost function until it reaches an minimum at $\frac{\partial L(f(x;\theta),y)}{\partial x} = 0$, then $x = x^{\ast}$ is called a "critical point". In general, the critical point at $\frac{\partial L(f(x;\theta),y)}{\partial x} = 0$ can be either a local minimum, a local maximum or a saddle point. To figure out the exact critical point at $x$, the second order partial derivative $\frac{\partial}{\partial x_{i} \partial x_{j}}L$ of the loss function $L$ at point $x$ can be calculated. It measures the curvature of the function at that point and tells how the first derivative (the slope) will vary when the input is changed, i.e. when moving along the cost function. The second
derivative determines curvature depending on its sign as well as the slope $\frac{\partial L}{\partial x}$ to the left and to the right of point $x$. <br />
When the input is high dimensional, there exists several first and second order derivatives for a function $L$ which can be packed into matrices to ease the search for critical points. Let $f$ be a real vector-valued function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ consisting of $m$ functions $f_{1},\dots,f_{m}: \mathbb{R}^{n} \rightarrow \mathbb{R}$, then
the Jacobian matrix is defined as follows: <br />
$J \in \mathbb{R}^{m\times n}$, $J_{i,j} :=$ $\frac{\partial f_{i}}{\partial x_{j}}.$ 
    
The first order optimization methods use the Jacobian matrix, i.e. the gradient of $f$ ($\nabla f$), to optimize the parameters of the neural models, whereas the second order optimization methods, e.g. Newton method, make use of
the Hessian matrix $H$ defined as follows: 
    
    $H \in \mathbb{R}^{n\times n}$, $H(f)(x)_{i,j} :=$ $\frac{\partial}{\partial x_{i} \partial x_{j}}f(x).$ <br \>
The Hessian matrix encompasses many second order derivatives that hint at the possible directions $d$ which can be taken by the gradient at point $x$ to move along the cost function. Each second order derivative in direction $d$ is represented by $d^{T}H(f)(x)d$. 

### Conditioning <br />
The condition number of a Hessian $H$ is denoted by $\mathcal{K}(H) = |\frac{\lambda_{max}}{\lambda_{min}}|$, 
where $\lambda_{max}$ and $\lambda_{min}$ are respectively the largest and the smallest eigenvalues of $H$. $\mathcal{K}(H)$ at a point $x$ measures the change of the second derivatives in all directions. 


### Ill-Conditioning <br />
The Hessian matrix $H$ is called ill-conditioned if it has a poor condition number, e.g. a very high $\mathcal{K}(H)$. In this case, gradient descent looses its ability to determine the direction of descent $d$ to achieve the minimum fast and correctly. In addition, a poorly-conditioned Hessian leads to problems when the gradient-based optimization method wants to choose a suitable step size to move to the next point. More precisely, an ill-conditioned Hessian gives rise to strong curvatures on the cost function surface and gradient descent adapts to these changes by taking smaller steps towards the minimum. If the minimum is far from the gradient, small step sizes will slow down the learning process. <br />
To solve the challenges caused by ill-conditioned Hessian matrices, Newton method has been proposed. Newton method (introduced earlier in chapter 4) is computed using the inverse of the hessian matrix. Accordingly, if the Hessian is strongly ill-conditioned, its inverse is also affected by the poor conditioning. Since the computation of a Hessian matrix as well as its inverse is computationally costly and requires a huge amount of storage space, the Newton method must be modified to be applied in the optimization process. The modification of the Newton method requires an approximation of the Hessian and its inverse without the need to calculate them exactly. The trick is to initially approximate the Newton method by a second-order Taylor expansion, then calculating the minimum of this approximation at $x^{\ast}$ and move towards this minimum. The approximation procedure is repeated until convergence. This iterative method is called the "Conjugate Gradients" method (see [Training Optimization 2](insert link later)). Nevertheless, Newton method is not widely applied in the context of neural networks because it is computationally expensive and it gets easily attracted to saddle points which may stop the optimization process. A second mitigation technique to overcome the problems of an ill-conditioned matrix is to adapt the "Momentum Algorithm" by ... dazu, ganz kurz was sagen(that will be explained in details in section 8.3).

### Local Minima
The cost function of a neural network is nonconvex presenting a single global minimum and a large number of local minima. Fortunately, the proliferation of local minima is not problematic due to the "non-identifiability" of neural networks. The "non-identifiability" property is a statistical property that declares that if we change the parameters of a model in different ways, we will get an equivalent neural model. *Weight Space Symmetry* illustrates the "non-identifiability" of neural networks by showing that swapping the weights arriving to two different neurons in a (hidden) layer as well as the weights leaving these neurons yields equivalent solution. Equivalent models can also be a result of multiplying weights and biases arriving to neurons by a number $\alpha$ and multiplying the weights and biases leaving these neurons by $\frac{1}{\alpha}$ as well.
The equivalence in models means also an equivalence of local minima that share the same cost value on the loss function because every local minimum will be situated at the bottom of a hyperbola shared by many local minima having the same cost value.
The proliferation of local minima is therefore not the major problem in the optimization process of deep networks. However, it becomes challenging when the cost value of a big number of local minima deviate strongly from the cost value of the global minimum, i.e. when the cost value of these local minima is much greater than the global loss. In this case, there is the risk that the learning process is not able to generalize well to new data that was not involved in training. 
Fortunately, the majority of local minima presents low loss value if the architecture of the deep model is chosen appropriately. In addition, it is rather sufficient to find a convenient local minimum that generalizes well instead of finding the global minimum to update the model's parameters.

### Plateaus, Saddle Points and other Flat Regions
Along with local minima, saddle points are widely spread on the surface of cost functions of deep networks due to its nonconvex shape. A saddle point can be depicted as a local minimum when looking to it from above, whereas it is local maximum if the point of view is located below the point. The first and second order optimization methods deal differently with saddle points. When first order optimization method, e.g. the Gradient Descent algorithm, approaches a saddle point, it often decreases the gradient and moves with small steps downhill to escape the critical point. However, the second order optimization methods, e.g. Newton method, face challenges when dealing with saddle points on the surface of loss functions. They recognize the saddle point as a critical point with a zero gradient ($\frac{\partial L}{\partial x}$) and may stop the optimization at this point instead of further descending to a minimal cost function value. In addition, deep neural networks of high dimensional spaces show that saddle-points are much more proliferated than any other extremum. This fact amplifies the challenge for the second order optimization methods to deal with saddle points.
To mitigate this problem, a method called the "saddle-free Newton method" was proposed to help the second order optimizer to quickly escape the saddle point. At a saddle point, the Hessian contains at least one negative eigenvalue and one positive eigenvalue because this critical point can be interpreted as local minimum and maximum depending from where it is perceived. The "saddle-free Newton method" calculates the absolute value of the Hessian matrix and adds a scaling parameter $\alpha$ to the eigenvalues of the Hessian. These adjustments of the Hessian matrix ensure that all its eigenvalues are positive similarly to the case of a local minimum. Consequently, the Hessian will treat this point as a local minimum and continue descending the loss function. 

In addition to saddle-points, plateaus and flat regions on the surface of the loss function cause problems when optimizing the training process because they are considered critical points ($\frac{\partial L}{\partial x} = 0$). If these flat regions have low cost value, they can be considered as a flat local minimum with no drawbacks. However, the problem is when these regions have high cost values because they will slow down the learning process since the algorithm adapts to make smaller steps when it recognizes such a minimum. Unfortunately, this problem applies mostly to nonconvex functions and it does not have any mitigation technique.

### Cliffs
A cliff is a region that undergo a sharp fall or a sharp rise depending on the location of perception. In both cases, it is dangerous to slide or to climb the cliff, and it is especially challenging to calculate the derivatives at such critical points because the gradient may surpass the cliff region to reach a point far away. The reason of such behavior is that the gradient takes only the direction $d$ and not the step size into consideration when it moves forward. A mitigation technique for this unwanted behavior is to use "Gradient Clipping Heuristic" that reduces the step size to prohibit the gradient to jump over the cliff region.

### Long-Term Dependencies
Neural networks process a weighted sum of the input vector as well as other operations over multiple layers forth and back. When the network is very deep, e.g. recurrent neural network, the computation will result in a deep computational graph. During the computation, vanishing and exploding gradient descent appear. In the vanishing gradient descent situation, the gradients cannot decide in which direction to move to get into a convenient low cost value of the loss function. On the other hand, an exploding gradient descent makes the learning process inconsistent. A commonly used mitigation technique for both challenges is to drop uninteresting features in the input vector using the power method. The following example illustrates how this method works. Suppose that a path of the computational graph applies a repeated multiplication with a matrix  $\textbf{\mathbf{W}}$, where $\textbf{\mathbf{W}} = \textbf{\mathbf{V}}diag(\lambda)\textbf{\mathbf{V}}^{-1}$ is the eigendecomposition of $\textbf{\mathbf{W}}$. After $t$ multiplication steps, there are $\textbf{\mathbf{W}}^{t}$ multiplications and the eigendecomposition becomes $\textbf{\mathbf{W}}^{t} = \textbf{\mathbf{V}}diag(\lambda)^{t}\textbf{\mathbf{V}}^{-1}$. The vanishing and exploding gradient descent problem arises from scaling $diag(\lambda)^{t}$. Here, the power method comes into play to detect the largest eigenvalue $\lambda_{i}$ of $\textbf{\mathbf{W}}$ and its eigenvector and accordingly to rule out all components that are orthogonal to $\textbf{\mathbf{W}}$ (clutter features).


### Poor Correspondence between Local and Global Structure
The presented mitigation techniques so far solved the optimization problem at a single point on the loss function in order to arrive to a low cost value. Although these methodologies improve the optimization process, it remains questionable whether the reached low cost value is sufficiently low with respect to other low values. Another open question is whether this low cost value drives the calculation of the gradient into a much lower cost value or not. For these questions, there are no direct answers, rather, experts advise to employ some heuristics that might help. One heuristic is to force the gradient to start at good points on the loss function to ensure that it will converge to a convenient minimum quickly. Another beneficial strategy is to set the target at a low cost value that generalizes well instead of seeking the global minimum on the loss function. Noteworthy is that the surrogate loss function presented earlier in section 8.1 is usually used to do all these computations and optimizations instead of the true loss function.

## 8.3 Basic Algorithms
A neural network changes its parameters to reduce the loss arising from the difference between the estimated and true outputs. This optimization process is done using optimization algorithms. In this section, three algorithms will be presented.
### Stochastic Gradient Descent
The most popular optimization algorithm is the Stochastic Gradient Descent (SGD). SGD requires initial parameters $\theta$ as well as learning rates $\epsilon_{k}$ for $k \in \mathbb{N}$ which adapt to the changes made by the algorithm at each iteration $k$. First, SGD picks a minibatch consisting of $m$ examples from the training set $\{\left(x^{(1)}, y^{(1)}\right), \dots, \left(x^{(m)}, y^{(m)}\right)\}$. On this minibatch, it computes a gradient estimate denoted as  $\hat{g} = \frac{1}{m} \nabla_{\theta} \sum_{i} L\left(f(x^{(i)}; \theta), y^{(i)}\right) $ and then applies the update step to the model's parameters $\theta = \theta - \epsilon_{k} \hat{g}$. At each iteration $k$, a new $\epsilon_{k}$ is calculated and the algorithm runs until the convergence criterion is met, i.e. until a convenient low cost value is reached. In the SGD algorithm, the adaptive learning rate $\epsilon_{k}$ is essential because it determines the rate by which the model have to change the parameters according to the loss function. The suitable learning rate can be chosen either by trial and error or by depicting the learning curve over time. Usually, $\epsilon_{k}$ decreases over time and in practice, a ratio $\alpha = \frac{k}{\tau}$ is defined to let $\epsilon_{k}$ decrease linearly until iteration $\tau$ according to the following formula: $$\epsilon_{k} = (1 - \alpha) \epsilon_{0} + \alpha \epsilon_{\tau},$$ where $\tau$ is the number of iterations needed to make few hundred passes through the neural network, $\epsilon_{\tau} = \frac{\epsilon_{0}}{100}$  and $\epsilon_{0}$ is the best performing $\epsilon_{k}$ in the first iterations. SGD popularity is also related to the fact that it allows convergence even with a huge number of training examples. It needs $J(\theta) - \min_{\theta} J(\theta)$ to calculate the excess error for convergence. The excess error measures the difference between current cost function and the minimum cost value reached by optimal parameters $\theta$. If SGD is applied to a convex problem, the excess error is $\mathcal{O}(\frac{1}{\sqrt{k}})$ after k iterations. Moreover, if it is applied to a strongly convex problem, the excess error becomes $\mathcal{O}(\frac{1}{k})$ after k iterations.



### Momentum

The momentum algorithm is another optimization algorithm used during neural network training. In physics, the momentum is defined as $ momentum = mass \times velocity$, however, in the context of optimization, the mass is ignored because it is assumed to be the unit mass. The momentum algorithm adds to the gradient descent method a velocity parameter, also called momentum, to determine how fast should the descent be on the surface of the cost function. The velocity parameter introduced in the momentum algorithm improves the performance of gradient descent over the SGD. SGD converges towards a low cost value by estimating the gradient of each step and the step size is determined by the adaptive learning rate $\epsilon_{k}$. If the region on the cost function shows high curvature or in the case of a small/noisy gradient, SGD will take very small step sizes and the learning becomes slow. However, the momentum algorithm recognizes such regions and applies an additional force to the gradient descent to accelerate the learning along the cost function. While descending, the velocity increases due to the applied force and the gradient descent becomes faster. Due to the increased velocity, the gradient can experience an oscillation on two sides of a valley on the loss function. Therefore, it is important to have another force to help the gradient to stop at the local minimum. This force is called “riscous drag”, it calculates the negative of the velocity to resist to the force accelerating the gradient on the loss function. The momentum algorithm requires an adaptive learning rate $\epsilon$, an adaptive momentum parameter $\alpha in [0,1)$, an initial parameter $\theta$ and an initial velocity $v$. The velocity $v$ determines the direction and the speed by which the point on the loss function $\theta$ must move(do they mean a point?). This velocity is the average of past gradients that is decreasing quickly. Thus, the momentum algorithm takes into consideration previously calculated gradients and use them in the next move. To determine by how much the contributions of previous gradients will affect the current velocity, the momentum parameter $\alpha$  is used. In practice, $\alpha$ is set to $\{0.5, 0.9, 0.99\}$ and it increases over time. Similar to SGD, the momentum picks a minibatch of $m$ examples from the training set, then computes the gradient estimate. Though, the momentum algorithm updates the velocity $v = \alpha v - \epsilon g$ and applies the update to the point on the curve $\theta = \theta + v$. The momentum algorithm runs until a convenient local minimum is reached.

### Nesterov Momentum


The Nesterov momentum is a third algorithm that can be used to optimize the training process of neural networks. It is an extension of the momentum algorithm because it adds a correction factor to the momentum. This correction is applied to the gradient estimation step by estimating $g = … $ instead of $g=..$, where $\theta + \alpha v$ is obtained after applying the momentum update step. The Nesterov momentum algorithm adds only one modification to the momentum algorithm in the gradient estimate step $g$.

## 8.4 Parameter Initialization Strategies

Training algorithms for deep learning are usually iterative. That means the user has to specify an initial point. Initial point affects the convergence, the speed of convergence and if we converge to a point with high or low cost. This last aspect is important as points of comparable cost can have different generalization error and the goal of training is to minimize the generalization error. 

Most initialization strategies are based on achieving good properties when the network isinitialized. There is no good understanding of how these properties are preserved during training. Certainly known is only that the initial parameters need to break symmetry between different units, which means that hidden units with same activation function and connection to same input parameters must have different initial parameters. This motivates to random initialization. <br /> 
More specifically, the weights are initialized randomly. The values are drawn either from a Gaussian or uniform distribution. The scale of the initial distribution has a large effect on the outcome, it influences optimization and generalization. Larger weights lead to stronger symmetry-breaking effect, but too large weights can cause exploding values during forward or backward-propagation or saturation of the activation function. Some heuristic initialization methods that are used in practice are: <br />

1. Sample each weight from $U(-\frac{1}{\sqrt{m}}, \frac{1}{\sqrt{m}})$, where m is the number of input layers. <br />
2. Normalized initialization: $W_{i, j} \sim U(-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}})$ <br />
3. Initialize to random orthogonal matrices with gain factor g that needs to be carefully chosen. <br />
4. Use sparse initialization: each unit is initialized to have exactly k nonzero weights. <br /> 
The second approach compromises between having the same activation variance and same gradient variance among all layers.<br /> 
An advantage of sparse initialization over approach 1. and 2. is that it does not scale with the number of inputs or outputs. An disadvantage is that it imposed a large prior on weights with large values.

Optimal criteria for initial weights do not lead to optimal performance. That is why in practice the it is useful to treat initial weights as hyperparameters and to treat the initial scale of the weights and whether to use sparse or dense initialization as hyperparameter aswell if not too costly.

The approach for setting the biases must be coordinated with the approach for setting the weights. Setting the biases to zero is compatible with most weight initialization schemes. E.g. it is important to set the biases to nonzero weights, if a unit controls whether other units are able to participate in a function or too avoid to much saturation at initialization time.

It is also possible to initialize model parameters using machine learning. This approach is not covered here.


 
# Questions

**Q:** Can you give an example for how networks can be nonidentifiable? 
Answer: For example by perturbing ingoing and outgoing weight vectors of two units in the same layer of a neural net we get two identical networks. This kind of nonidentifiability is called weight space symmetry. 

**Q:** Can you explain the concept of **vicious drag**? 
Answer: Vicious drag is the force in opposite direction of velocity that slows down a moving particle.

**Q:** Can you name advantages/disadvantages of a big/small momentum parameter for SGD?
Answer: The momentum parameter $$\alpha$$ in SGD with mom entum determines how fast the effect of previously compouted gradients decreases. As $$\alpha$$ gets smaller the previous gradients have exponentially less impact on the new direction of descent and the method approaches a usual SGD. The choice of $$\alpha$$ is highly specific to the application. Common values for $$\alpha$$ are 0.5, 0.9, and 0.99.

**Q**: Can you explain the Saddle-free Newton Method? 
Answer: The Newton Method tends to get stuck in saddle points. This can be fixed by employing a so called trust region approach. This approach consists of adding a constant value $$\alpha$$ to the eigenvalues of the Hessian matrix. Choosing $$\alpha$$ sufficiently large prevents the Newton Method from getting stuck in saddle points. 

**Q**: Why do we gradually decrease the learning rate over time in SGD? 
Answer: Since the gradient estimator produces a constant source of noise which leads to the gradients not vanishing entirely. 

**Q**: Can you give an example for exploding/vanishing gradients?
Answer: Repeated multiplications with the same weight Matrix $$W$$ can lead to vanishing or exploding gradients. An example of this can be found on Slide 38.

**Q**: How does the use of Nesterov Momentum improve the error rates?
Answer: For batch gradient descent on a convex function it can be shown that the use of Nesterov momentum improves the convergence of the excess error from $$O(1/k)$$ to $$O(1/k^2)$$. For SGD there are no such theoretical results.

**Q**: Can you come up with an example where shuffling the data is important before choosing the minibatches? 
Answer: Some data sets are arranged in a way such that subsequent examples are highly correlated which leads to the gradient estimations to not be independent. For example a list of blood sample test results might contain blood samples from single patients at different points in time in consecutive blocks. 
